# handler/p115_service.py
import logging
import requests
import random
import os
import json
import re
import threading
import time
import config_manager
import constants
from database import settings_db
from database.connection import get_db_connection
import handler.tmdb as tmdb
import utils
try:
    from p115client import P115Client
except ImportError:
    P115Client = None

logger = logging.getLogger(__name__)

# ======================================================================
# â˜…â˜…â˜… 115 OpenAPI å®¢æˆ·ç«¯ (ä»…ç®¡ç†æ“ä½œï¼šæ‰«æ/åˆ›å»ºç›®å½•/ç§»åŠ¨æ–‡ä»¶) â˜…â˜…â˜…
# ======================================================================
class P115OpenAPIClient:
    """ä½¿ç”¨ Access Token è¿›è¡Œç®¡ç†æ“ä½œ"""
    def __init__(self, access_token):
        if not access_token:
            raise ValueError("Access Token ä¸èƒ½ä¸ºç©º")
        self.access_token = access_token.strip()
        self.base_url = "https://proapi.115.com"
        self.headers = {
            "Authorization": f"Bearer {self.access_token}",
            "User-Agent": "Emby-toolkit/1.0 (OpenAPI)"
        }

    def get_user_info(self):
        url = f"{self.base_url}/open/user/info"
        try:
            return requests.get(url, headers=self.headers, timeout=10).json()
        except Exception as e:
            return {"state": False, "message": str(e)}

    def fs_files(self, payload):
        """è·å–æ–‡ä»¶åˆ—è¡¨ - çº¯å‡€ OpenAPI ç‰ˆ (ä¸¥æ ¼è¿”å›å®˜æ–¹åŸå§‹å­—æ®µ)"""
        url = f"{self.base_url}/open/ufile/files"
        params = {"show_dir": 1, "limit": 1000, "offset": 0}
        if isinstance(payload, dict): params.update(payload)
        
        try:
            return requests.get(url, params=params, headers=self.headers, timeout=30).json()
        except Exception as e:
            return {"state": False, "error_msg": str(e)}

    def fs_files_app(self, payload): return self.fs_files(payload)

    def fs_mkdir(self, name, pid):
        url = f"{self.base_url}/open/folder/add"
        resp = requests.post(url, data={"pid": str(pid), "file_name": str(name)}, headers=self.headers).json()
        if resp.get("state") and "data" in resp: resp["cid"] = resp["data"].get("file_id")
        return resp

    def fs_move(self, fid, to_cid):
        return requests.post(f"{self.base_url}/open/ufile/move", data={"file_ids": str(fid), "to_cid": str(to_cid)}, headers=self.headers).json()

    def fs_rename(self, fid_name_tuple):
        return requests.post(f"{self.base_url}/open/ufile/update", data={"file_id": str(fid_name_tuple[0]), "file_name": str(fid_name_tuple[1])}, headers=self.headers).json()

    def fs_delete(self, fids):
        fids_str = ",".join([str(f) for f in fids]) if isinstance(fids, list) else str(fids)
        return requests.post(f"{self.base_url}/open/ufile/delete", data={"file_ids": fids_str}, headers=self.headers).json()


# ======================================================================
# â˜…â˜…â˜… 115 Cookie å®¢æˆ·ç«¯ (ä»…æ’­æ”¾ï¼šè·å–ç›´é“¾) â˜…â˜…â˜…
# ======================================================================
class P115CookieClient:
    """ä½¿ç”¨ Cookie è¿›è¡Œæ’­æ”¾æ“ä½œ"""
    def __init__(self, cookie_str):
        if not cookie_str:
            raise ValueError("Cookie ä¸èƒ½ä¸ºç©º")
        self.cookie_str = cookie_str.strip()
        self.webapi = None
        if P115Client:
            try:
                self.webapi = P115Client(self.cookie_str)
            except Exception as e:
                logger.warning(f"  âš ï¸ Cookie å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                raise

    def download_url(self, pick_code, user_agent=None):
        """è·å–ç›´é“¾ (ä»… Cookie å¯ç”¨)"""
        if self.webapi:
            try:
                url_obj = self.webapi.download_url(pick_code, user_agent=user_agent)
                if url_obj: return str(url_obj)
            except Exception as e:
                logger.warning(f"  âš ï¸ Cookie ç›´é“¾è·å–å¤±è´¥: {e}")
        return None

    def get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯ (ä»…ç”¨äºéªŒè¯)"""
        if self.webapi:
            try:
                # Cookie æ¨¡å¼è·å–ç”¨æˆ·ä¿¡æ¯çš„æ–¹å¼æœ‰é™
                return {"state": True, "data": {"user_name": "Cookieç”¨æˆ·"}}
            except:
                pass
        return None


# ======================================================================
# â˜…â˜…â˜… 115 æœåŠ¡ç®¡ç†å™¨ (åˆ†ç¦»ç®¡ç†/æ’­æ”¾å®¢æˆ·ç«¯) â˜…â˜…â˜…
# ======================================================================
class P115Service:
    """ç»Ÿä¸€ç®¡ç† OpenAPI å’Œ Cookie å®¢æˆ·ç«¯"""
    _instance = None
    _lock = threading.Lock()
    
    # å®¢æˆ·ç«¯ç¼“å­˜
    _openapi_client = None
    _cookie_client = None
    _token_cache = None
    _cookie_cache = None
    
    _last_request_time = 0

    @classmethod
    def get_openapi_client(cls):
        """è·å–ç®¡ç†å®¢æˆ·ç«¯ (OpenAPI)"""
        config = get_config()
        token = config.get(constants.CONFIG_OPTION_115_TOKEN, "").strip()
        
        if not token:
            return None

        with cls._lock:
            if cls._openapi_client is None or token != cls._token_cache:
                try:
                    cls._openapi_client = P115OpenAPIClient(token)
                    cls._token_cache = token
                    logger.info("  ğŸš€ [115] OpenAPI å®¢æˆ·ç«¯å·²åˆå§‹åŒ– (Token æ¨¡å¼)")
                except Exception as e:
                    logger.error(f"  âŒ 115 OpenAPI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    cls._openapi_client = None
            
            return cls._openapi_client

    @classmethod
    def get_cookie_client(cls):
        """è·å–æ’­æ”¾å®¢æˆ·ç«¯ (Cookie)"""
        config = get_config()
        cookie = config.get(constants.CONFIG_OPTION_115_COOKIES, "").strip()
        
        if not cookie:
            return None

        with cls._lock:
            if cls._cookie_client is None or cookie != cls._cookie_cache:
                try:
                    cls._cookie_client = P115CookieClient(cookie)
                    cls._cookie_cache = cookie
                    logger.info("  ğŸš€ [115] Cookie å®¢æˆ·ç«¯å·²åˆå§‹åŒ– (æ’­æ”¾æ¨¡å¼)")
                except Exception as e:
                    logger.error(f"  âŒ 115 Cookie å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    cls._cookie_client = None
            
            return cls._cookie_client

    @classmethod
    def get_client(cls):
        """
        è·å–ä¸¥æ ¼åˆ†ç¦»å®¢æˆ·ç«¯ï¼š
        ç®¡ç†æ“ä½œ -> å¼ºåˆ¶èµ° OpenAPI
        æ’­æ”¾æ“ä½œ -> å¼ºåˆ¶èµ° Cookie
        """
        openapi = cls.get_openapi_client()
        cookie = cls.get_cookie_client()
        
        if not openapi and not cookie:
            return None

        class StrictSplitClient:
            def __init__(self, openapi_client, cookie_client):
                self._openapi = openapi_client
                self._cookie = cookie_client

            def _check_openapi(self):
                if not self._openapi:
                    raise Exception("æœªé…ç½® 115 Token (OpenAPI)ï¼Œæ— æ³•æ‰§è¡Œç®¡ç†æ“ä½œ")

            def get_user_info(self):
                if self._openapi: return self._openapi.get_user_info()
                if self._cookie: return self._cookie.get_user_info()
                return None

            def fs_files(self, payload):
                self._check_openapi()
                return self._openapi.fs_files(payload)

            def fs_files_app(self, payload):
                self._check_openapi()
                return self._openapi.fs_files_app(payload)

            def fs_mkdir(self, name, pid):
                self._check_openapi()
                return self._openapi.fs_mkdir(name, pid)

            def fs_move(self, fid, to_cid):
                self._check_openapi()
                return self._openapi.fs_move(fid, to_cid)

            def fs_rename(self, fid_name_tuple):
                self._check_openapi()
                return self._openapi.fs_rename(fid_name_tuple)

            def fs_delete(self, fids):
                self._check_openapi()
                return self._openapi.fs_delete(fids)

            def download_url(self, pick_code, user_agent=None):
                if not self._cookie:
                    raise Exception("æœªé…ç½® 115 Cookieï¼Œæ— æ³•è·å–æ’­æ”¾ç›´é“¾")
                return self._cookie.download_url(pick_code, user_agent)

        # å…¨å±€é™æµé€»è¾‘
        with cls._lock:
            try:
                interval = float(get_config().get(constants.CONFIG_OPTION_115_INTERVAL, 5.0))
            except (ValueError, TypeError):
                interval = 5.0
            
            current_time = time.time()
            elapsed = current_time - cls._last_request_time
            if elapsed < interval:
                time.sleep(interval - elapsed)
            cls._last_request_time = time.time()

        return StrictSplitClient(openapi, cookie)
    
    @classmethod
    def get_cookies(cls):
        """è·å– Cookie (ç”¨äºç›´é“¾ä¸‹è½½ç­‰)"""
        config = get_config()
        return config.get(constants.CONFIG_OPTION_115_COOKIES)
    
    @classmethod
    def get_token(cls):
        """è·å– Token (ç”¨äº API è°ƒç”¨)"""
        config = get_config()
        return config.get(constants.CONFIG_OPTION_115_TOKEN)


# ======================================================================
# â˜…â˜…â˜… æ–°å¢ï¼š115 ç›®å½•æ ‘ DB ç¼“å­˜ç®¡ç†å™¨ â˜…â˜…â˜…
# ======================================================================
class P115CacheManager:
    @staticmethod
    def get_cid(parent_cid, name):
        """ä»æœ¬åœ°æ•°æ®åº“è·å– CID (æ¯«ç§’çº§)"""
        if not parent_cid or not name: return None
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(
                        "SELECT id FROM p115_filesystem_cache WHERE parent_id = %s AND name = %s", 
                        (str(parent_cid), str(name))
                    )
                    row = cursor.fetchone()
                    return row['id'] if row else None
        except Exception as e:
            logger.error(f"  âŒ è¯»å– 115 DB ç¼“å­˜å¤±è´¥: {e}")
            return None

    @staticmethod
    def save_cid(cid, parent_cid, name):
        """å°† CID å­˜å…¥æœ¬åœ°æ•°æ®åº“ç¼“å­˜"""
        if not cid or not parent_cid or not name: return
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO p115_filesystem_cache (id, parent_id, name)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (parent_id, name)
                        DO UPDATE SET id = EXCLUDED.id, updated_at = NOW()
                    """, (str(cid), str(parent_cid), str(name)))
                    conn.commit()
        except Exception as e:
            logger.error(f"  âŒ å†™å…¥ 115 DB ç¼“å­˜å¤±è´¥: {e}")

    @staticmethod
    def get_cid_by_name(name):
        """ä»…é€šè¿‡åç§°æŸ¥æ‰¾ CID (é€‚ç”¨äºå¸¦æœ‰ {tmdb=xxx} çš„å”¯ä¸€ä¸»ç›®å½•)"""
        if not name: return None
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT id FROM p115_filesystem_cache WHERE name = %s LIMIT 1", (str(name),))
                    row = cursor.fetchone()
                    return row['id'] if row else None
        except Exception as e:
            return None

    @staticmethod
    def delete_cid(cid):
        """ä»ç¼“å­˜ä¸­ç‰©ç†åˆ é™¤è¯¥ç›®å½•åŠå…¶å­ç›®å½•çš„è®°å½•"""
        if not cid: return
        try:
            with get_db_connection() as conn:
                with conn.cursor() as cursor:
                    # åˆ é™¤è‡ªèº«ä»¥åŠä»¥å®ƒä¸ºçˆ¶ç›®å½•çš„å­é¡¹
                    cursor.execute("DELETE FROM p115_filesystem_cache WHERE id = %s OR parent_id = %s", (str(cid), str(cid)))
                    conn.commit()
        except Exception as e:
            logger.error(f"  âŒ æ¸…ç† 115 DB ç¼“å­˜å¤±è´¥: {e}")

def get_config():
    return config_manager.APP_CONFIG

class SmartOrganizer:
    def __init__(self, client, tmdb_id, media_type, original_title):
        self.client = client
        self.tmdb_id = tmdb_id
        self.media_type = media_type
        self.original_title = original_title
        self.api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)

        self.studio_map = settings_db.get_setting('studio_mapping') or utils.DEFAULT_STUDIO_MAPPING
        self.keyword_map = settings_db.get_setting('keyword_mapping') or utils.DEFAULT_KEYWORD_MAPPING
        self.rating_map = settings_db.get_setting('rating_mapping') or utils.DEFAULT_RATING_MAPPING
        self.rating_priority = settings_db.get_setting('rating_priority') or utils.DEFAULT_RATING_PRIORITY

        self.raw_metadata = self._fetch_raw_metadata()
        self.details = self.raw_metadata
        raw_rules = settings_db.get_setting(constants.DB_KEY_115_SORTING_RULES)
        self.rules = []
        
        if raw_rules:
            if isinstance(raw_rules, list):
                self.rules = raw_rules
            elif isinstance(raw_rules, str):
                try:
                    self.rules = json.loads(raw_rules)
                except Exception as e:
                    logger.error(f"  âŒ è§£æ 115 åˆ†ç±»è§„åˆ™å¤±è´¥: {e}")
                    self.rules = []

    def _fetch_raw_metadata(self):
        """
        è·å– TMDb åŸå§‹å…ƒæ•°æ® (ID/Code)ï¼Œä¸è¿›è¡Œä»»ä½•ä¸­æ–‡è½¬æ¢ã€‚
        """
        if not self.api_key: return {}

        data = {
            'genre_ids': [],
            'country_codes': [],
            'lang_code': None,
            'company_ids': [],
            'network_ids': [],
            'keyword_ids': [],
            'rating_label': 'æœªçŸ¥' # åˆ†çº§æ˜¯ç‰¹ä¾‹ï¼Œå¿…é¡»è®¡ç®—å‡ºæ ‡ç­¾æ‰èƒ½åŒ¹é…
        }

        try:
            raw_details = {}
            if self.media_type == 'tv':
                raw_details = tmdb.get_tv_details(
                    self.tmdb_id, self.api_key,
                    append_to_response="keywords,content_ratings,networks"
                )
            else:
                raw_details = tmdb.get_movie_details(
                    self.tmdb_id, self.api_key,
                    append_to_response="keywords,release_dates"
                )

            if not raw_details: return {}

            # 1. åŸºç¡€ ID/Code æå–
            data['genre_ids'] = [g.get('id') for g in raw_details.get('genres', [])]
            data['country_codes'] = [c.get('iso_3166_1') for c in raw_details.get('production_countries', [])]
            if not data['country_codes'] and raw_details.get('origin_country'):
                data['country_codes'] = raw_details.get('origin_country')

            data['lang_code'] = raw_details.get('original_language')

            data['company_ids'] = [c.get('id') for c in raw_details.get('production_companies', [])]
            data['network_ids'] = [n.get('id') for n in raw_details.get('networks', [])] if self.media_type == 'tv' else []

            # 2. å…³é”®è¯ ID æå–
            kw_container = raw_details.get('keywords', {})
            raw_kw_list = kw_container.get('keywords', []) if self.media_type == 'movie' else kw_container.get('results', [])
            data['keyword_ids'] = [k.get('id') for k in raw_kw_list]

            # 3. åˆ†çº§è®¡ç®— 
            data['rating_label'] = utils.get_rating_label(
                raw_details,
                self.media_type,
                self.rating_map,
                self.rating_priority
            )

            # è¡¥å……æ ‡é¢˜æ—¥æœŸä¾›é‡å‘½å
            data['title'] = raw_details.get('title') or raw_details.get('name')
            date_str = raw_details.get('release_date') or raw_details.get('first_air_date')
            data['date'] = date_str
            data['year'] = 0
            
            if date_str and len(str(date_str)) >= 4:
                try:
                    data['year'] = int(str(date_str)[:4])
                except: 
                    pass
            # è¡¥å……è¯„åˆ†ä¾›è§„åˆ™åŒ¹é…
            data['vote_average'] = raw_details.get('vote_average', 0)

            return data

        except Exception as e:
            logger.warning(f"  âš ï¸ [æ•´ç†] è·å–åŸå§‹å…ƒæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {}

    def _match_rule(self, rule):
        """
        è§„åˆ™åŒ¹é…é€»è¾‘ï¼š
        - æ ‡å‡†å­—æ®µï¼šç›´æ¥æ¯”å¯¹ ID/Code
        - é›†åˆå­—æ®µï¼ˆå·¥ä½œå®¤/å…³é”®è¯ï¼‰ï¼šé€šè¿‡ Label åæŸ¥ Config ä¸­çš„ ID åˆ—è¡¨ï¼Œå†æ¯”å¯¹ TMDb ID
        """
        if not self.raw_metadata: return False

        # 1. åª’ä½“ç±»å‹
        if rule.get('media_type') and rule['media_type'] != 'all':
            if rule['media_type'] != self.media_type: return False

        # 2. ç±»å‹ (Genres) - ID åŒ¹é…
        if rule.get('genres'):
            # rule['genres'] å­˜çš„æ˜¯ ID åˆ—è¡¨ (å¦‚ [16, 35])
            # self.raw_metadata['genre_ids'] æ˜¯ TMDb ID åˆ—è¡¨
            # åªè¦æœ‰ä¸€ä¸ªäº¤é›†å°±ç®—å‘½ä¸­
            rule_ids = [int(x) for x in rule['genres']]
            if not any(gid in self.raw_metadata['genre_ids'] for gid in rule_ids): return False

        # 3. å›½å®¶ (Countries) - Code åŒ¹é…
        if rule.get('countries'):
            # rule['countries'] å­˜çš„æ˜¯ Code (å¦‚ ['US', 'CN'])
            # åªåŒ¹é…ç¬¬ä¸€ä¸ªä¸»è¦å›½å®¶ï¼Œé¿å…åˆæ‹ç‰‡è¯¯åˆ¤ 
            current_countries = self.raw_metadata.get('country_codes', [])
            # è·å–åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå›½å®¶ä½œä¸ºä¸»è¦å›½å®¶
            primary_country = current_countries[0] if current_countries else None
            
            # å¦‚æœæ²¡æœ‰å›½å®¶ä¿¡æ¯ï¼Œæˆ–è€…ä¸»è¦å›½å®¶ä¸åœ¨è§„åˆ™å…è®¸çš„åˆ—è¡¨ä¸­ï¼Œåˆ™ä¸åŒ¹é…
            if not primary_country or primary_country not in rule['countries']:
                return False

        # 4. è¯­è¨€ (Languages) - Code åŒ¹é…
        if rule.get('languages'):
            if self.raw_metadata['lang_code'] not in rule['languages']: return False

        # 5. å·¥ä½œå®¤ (Studios) - Label -> ID åŒ¹é…
        if rule.get('studios'):
            # rule['studios'] å­˜çš„æ˜¯ Label (å¦‚ ['æ¼«å¨', 'Netflix'])
            # æˆ‘ä»¬éœ€è¦éå†è¿™äº› Labelï¼Œå» self.studio_map é‡Œæ‰¾å¯¹åº”çš„ ID
            target_ids = set()
            for label in rule['studios']:
                # æ‰¾åˆ°é…ç½®é¡¹
                config_item = next((item for item in self.studio_map if item['label'] == label), None)
                if config_item:
                    target_ids.update(config_item.get('company_ids', []))
                    target_ids.update(config_item.get('network_ids', []))

            # æ£€æŸ¥ TMDb çš„ company/network ID æ˜¯å¦åœ¨ target_ids ä¸­
            has_company = any(cid in target_ids for cid in self.raw_metadata['company_ids'])
            has_network = any(nid in target_ids for nid in self.raw_metadata['network_ids'])

            if not (has_company or has_network): return False

        # 6. å…³é”®è¯ (Keywords) - Label -> ID åŒ¹é…
        if rule.get('keywords'):
            target_ids = set()
            for label in rule['keywords']:
                config_item = next((item for item in self.keyword_map if item['label'] == label), None)
                if config_item:
                    target_ids.update(config_item.get('ids', []))

            # å…¼å®¹å­—ç¬¦ä¸²/æ•°å­— ID
            tmdb_kw_ids = [int(k) for k in self.raw_metadata['keyword_ids']]
            target_ids_int = [int(k) for k in target_ids]

            if not any(kid in target_ids_int for kid in tmdb_kw_ids): return False

        # 7. åˆ†çº§ (Rating) - Label åŒ¹é…
        if rule.get('ratings'):
            if self.raw_metadata['rating_label'] not in rule['ratings']: return False

        # 8. å¹´ä»½ (Year) 
        year_min = rule.get('year_min')
        year_max = rule.get('year_max')
        
        if year_min or year_max:
            current_year = self.raw_metadata.get('year', 0)
            
            # å¦‚æœè·å–ä¸åˆ°å¹´ä»½ï¼Œä¸”è®¾ç½®äº†å¹´ä»½é™åˆ¶ï¼Œåˆ™è§†ä¸ºä¸åŒ¹é…
            if current_year == 0: return False
            
            if year_min and current_year < int(year_min): return False
            if year_max and current_year > int(year_max): return False

        # 9. æ—¶é•¿ (Runtime) 
        # é€»è¾‘ï¼šç”µå½±å– runtimeï¼Œå‰§é›†å– episode_run_time (åˆ—è¡¨å–å¹³å‡æˆ–ç¬¬ä¸€ä¸ª)
        run_min = rule.get('runtime_min')
        run_max = rule.get('runtime_max')

        if run_min or run_max:
            current_runtime = 0
            if self.media_type == 'movie':
                current_runtime = self.details.get('runtime') or 0
            else:
                # å‰§é›†æ—¶é•¿é€šå¸¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ [45, 60]ï¼Œå–ç¬¬ä¸€ä¸ªä½œä¸ºå‚è€ƒ
                runtimes = self.details.get('episode_run_time', [])
                if runtimes and len(runtimes) > 0:
                    current_runtime = runtimes[0]

            # å¦‚æœè·å–ä¸åˆ°æ—¶é•¿ï¼Œä¸”è®¾ç½®äº†é™åˆ¶ï¼Œè§†ä¸ºä¸åŒ¹é…
            if current_runtime == 0: return False

            if run_min and current_runtime < int(run_min): return False
            if run_max and current_runtime > int(run_max): return False

        # 10. è¯„åˆ† (Min Rating) - æ•°å€¼æ¯”è¾ƒ
        if rule.get('min_rating') and float(rule['min_rating']) > 0:
            vote_avg = self.details.get('vote_average', 0)
            if vote_avg < float(rule['min_rating']):
                return False

        return True

    def get_target_cid(self):
        """éå†è§„åˆ™ï¼Œè¿”å›å‘½ä¸­çš„ CIDã€‚æœªå‘½ä¸­è¿”å› None"""
        for rule in self.rules:
            if not rule.get('enabled', True): continue
            if self._match_rule(rule):
                logger.info(f"  ğŸ¯ [115] å‘½ä¸­è§„åˆ™: {rule.get('name')} -> ç›®å½•: {rule.get('dir_name')}")
                return rule.get('cid')
        return None

    def _extract_video_info(self, filename):
        """
        ä»æ–‡ä»¶åæå–è§†é¢‘ä¿¡æ¯ (æ¥æº Â· åˆ†è¾¨ç‡ Â· ç¼–ç  Â· éŸ³é¢‘ Â· åˆ¶ä½œç»„)
        å‚è€ƒæ ¼å¼: BluRay Â· 1080p Â· X264 Â· DDP 7.1 Â· CMCT
        """
        info_tags = []
        name_upper = filename.upper()

        # 1. æ¥æº/è´¨é‡ (Source)
        source = ""
        if re.search(r'REMUX', name_upper): source = 'Remux'
        elif re.search(r'BLU-?RAY|BD', name_upper): source = 'BluRay'
        elif re.search(r'WEB-?DL', name_upper): source = 'WEB-DL'
        elif re.search(r'WEB-?RIP', name_upper): source = 'WEBRip'
        elif re.search(r'HDTV', name_upper): source = 'HDTV'
        elif re.search(r'DVD', name_upper): source = 'DVD'

        # â˜…â˜…â˜… ä¿®å¤ï¼šUHD è¯†åˆ« â˜…â˜…â˜…
        if 'UHD' in name_upper:
            if source == 'BluRay': source = 'UHD BluRay'
            elif not source: source = 'UHD'

        # 2. ç‰¹æ•ˆ (Effect: HDR/DV)
        effect = ""
        is_dv = re.search(r'(?:^|[\.\s\-\_])(DV|DOVI|DOLBY\s?VISION)(?:$|[\.\s\-\_])', name_upper)
        is_hdr = re.search(r'(?:^|[\.\s\-\_])(HDR|HDR10\+?)(?:$|[\.\s\-\_])', name_upper)

        if is_dv and is_hdr: effect = "HDR DV"
        elif is_dv: effect = "DV"
        elif is_hdr: effect = "HDR"

        if source:
            info_tags.append(f"{source} {effect}".strip())
        elif effect:
            info_tags.append(effect)

        # 3. åˆ†è¾¨ç‡ (Resolution)
        res_match = re.search(r'(2160|1080|720|480)[pP]', filename)
        if res_match:
            info_tags.append(res_match.group(0).lower())
        elif '4K' in name_upper:
            info_tags.append('2160p')

        # 4. ç¼–ç  (Codec)
        codec = ""
        if re.search(r'[HX]265|HEVC', name_upper): info_tags.append('H265')
        elif re.search(r'[HX]264|AVC', name_upper): info_tags.append('H264')
        elif re.search(r'AV1', name_upper): info_tags.append('AV1')
        elif re.search(r'MPEG-?2', name_upper): info_tags.append('MPEG2')
        # æ¯”ç‰¹ç‡æå– (Bit Depth) 
        bit_depth = ""
        bit_match = re.search(r'(\d{1,2})BIT', name_upper)
        if bit_match:
            bit_depth = f"{bit_match.group(1)}bit" # ç»Ÿä¸€æ ¼å¼ä¸ºå°å†™ bit

        # å°†ç¼–ç å’Œæ¯”ç‰¹ç‡ç»„åˆï¼Œæ¯”å¦‚ "H265 10bit" æˆ–å•ç‹¬ "H265"
        if codec:
            full_codec = f"{codec} {bit_depth}".strip()
            info_tags.append(full_codec)
        elif bit_depth:
            info_tags.append(bit_depth)

        # 5. éŸ³é¢‘ (Audio) - â˜…â˜…â˜… ä¿®å¤é‡ç‚¹ â˜…â˜…â˜…
        audio_info = []
        
        # (1) ä¼˜å…ˆåŒ¹é…å¸¦æ•°å­—çš„éŸ³è½¨ (2Audio, 3Audios) å¹¶ç»Ÿä¸€æ ¼å¼ä¸º "xAudios"
        # æ­£åˆ™è¯´æ˜: åŒ¹é…è¾¹ç•Œ + æ•°å­— + ç©ºæ ¼(å¯é€‰) + Audio + s(å¯é€‰) + è¾¹ç•Œ
        num_audio_match = re.search(r'\b(\d+)\s?Audios?\b', name_upper, re.IGNORECASE)
        if num_audio_match:
            # ç»Ÿä¸€æ ¼å¼åŒ–ä¸º: æ•°å­— + Audios (ä¾‹å¦‚: 2Audios)
            audio_info.append(f"{num_audio_match.group(1)}Audios")
        else:
            # (2) å¦‚æœæ²¡æœ‰æ•°å­—éŸ³è½¨ï¼Œå†åŒ¹é… Multi/Dual ç­‰é€šç”¨æ ‡ç­¾
            if re.search(r'\b(Multi|åŒè¯­|å¤šéŸ³è½¨|Dual-Audio)\b', name_upper, re.IGNORECASE):
                audio_info.append('Multi')

        # (3) å…¶ä»–å…·ä½“éŸ³é¢‘ç¼–ç 
        if re.search(r'ATMOS', name_upper): audio_info.append('Atmos')
        elif re.search(r'TRUEHD', name_upper): audio_info.append('TrueHD')
        elif re.search(r'DTS-?HD(\s?MA)?', name_upper): audio_info.append('DTS-HD')
        elif re.search(r'DTS', name_upper): audio_info.append('DTS')
        elif re.search(r'DDP|EAC3|DOLBY\s?DIGITAL\+', name_upper): audio_info.append('DDP')
        elif re.search(r'AC3|DD', name_upper): audio_info.append('AC3')
        elif re.search(r'AAC', name_upper): audio_info.append('AAC')
        elif re.search(r'FLAC', name_upper): audio_info.append('FLAC')
        elif re.search(r'OPUS', name_upper): audio_info.append('Opus')
        
        chan_match = re.search(r'\b(7\.1|5\.1|2\.0)\b', filename)
        if chan_match:
            audio_info.append(chan_match.group(1))
            
        if audio_info:
            info_tags.append(" ".join(audio_info))

        # æµåª’ä½“å¹³å°è¯†åˆ«
        # åŒ¹é… NF, AMZN, DSNP, HMAX, HULU, NETFLIX, DISNEY+, APPLETV+
        stream_match = re.search(r'\b(NF|AMZN|DSNP|HMAX|HULU|NETFLIX|DISNEY\+|APPLETV\+|B-GLOBAL)\b', name_upper)
        if stream_match:
            info_tags.append(stream_match.group(1))

        # 6. å‘å¸ƒç»„ (Release Group)
        group_found = False
        try:
            from tasks import helpers
            for group_name, patterns in helpers.RELEASE_GROUPS.items():
                for pattern in patterns:
                    try:
                        match = re.search(pattern, filename, re.IGNORECASE)
                        if match:
                            info_tags.append(match.group(0))
                            group_found = True
                            break
                    except: pass
                if group_found: break

            if not group_found:
                name_no_ext = os.path.splitext(filename)[0]
                match_suffix = re.search(r'-([a-zA-Z0-9]+)$', name_no_ext)
                if match_suffix:
                    possible_group = match_suffix.group(1)
                    if len(possible_group) > 2 and possible_group.upper() not in ['1080P', '2160P', '4K', 'HDR', 'H265', 'H264']:
                        info_tags.append(possible_group)
        except ImportError:
            pass

        return " Â· ".join(info_tags) if info_tags else ""

    def _rename_file_node(self, file_node, new_base_name, year=None, is_tv=False):
        # å…¼å®¹ OpenAPI é”®å
        original_name = file_node.get('fn') or file_node.get('n') or file_node.get('file_name', '')
        if '.' not in original_name: return original_name, None

        parts = original_name.rsplit('.', 1)
        name_body = parts[0]
        ext = parts[1].lower()

        is_sub = ext in ['srt', 'ass', 'ssa', 'sub', 'vtt', 'sup']
        lang_suffix = ""
        if is_sub:
            lang_keywords = [
                'zh', 'cn', 'tw', 'hk', 'en', 'jp', 'kr',
                'chs', 'cht', 'eng', 'jpn', 'kor', 'fre', 'spa',
                'default', 'forced', 'tc', 'sc'
            ]
            sub_parts = name_body.split('.')
            if len(sub_parts) > 1:
                last_part = sub_parts[-1].lower()
                if last_part in lang_keywords or '-' in last_part:
                    lang_suffix = f".{sub_parts[-1]}"

            if not lang_suffix:
                match = re.search(r'(?:\.|-|_|\s)(chs|cht|zh-cn|zh-tw|eng|jpn|kor|tc|sc)(?:\.|-|_|$)', name_body, re.IGNORECASE)
                if match:
                    lang_suffix = f".{match.group(1)}"

        tag_suffix = ""
        try:
            search_name = original_name
            if is_sub:
                if lang_suffix and name_body.endswith(lang_suffix):
                    clean_body = name_body[:-len(lang_suffix)]
                    search_name = f"{clean_body}.mkv"
                else:
                    search_name = f"{name_body}.mkv"

            video_info = self._extract_video_info(search_name)
            if video_info:
                tag_suffix = f" Â· {video_info}"
        except Exception as e:
            pass

        if is_tv:
            pattern = r'(?:s|S)(\d{1,2})(?:e|E)(\d{1,2})|Ep?(\d{1,2})|ç¬¬(\d{1,3})[é›†è¯]'
            match = re.search(pattern, original_name)
            if match:
                s, e, ep_only, zh_ep = match.groups()
                season_num = int(s) if s else 1
                episode_num = int(e) if e else (int(ep_only) if ep_only else int(zh_ep))

                s_str = f"S{season_num:02d}"
                e_str = f"E{episode_num:02d}"

                new_name = f"{new_base_name} - {s_str}{e_str}{tag_suffix}{lang_suffix}.{ext}"
                return new_name, season_num
            else:
                return original_name, None
        else:
            movie_base = f"{new_base_name} ({year})" if year else new_base_name
            new_name = f"{movie_base}{tag_suffix}{lang_suffix}.{ext}"
            return new_name, None

    def _scan_files_recursively(self, cid, depth=0, max_depth=3):
        all_files = []
        if depth > max_depth: return []
        try:
            time.sleep(1.5) 
            res = self.client.fs_files({'cid': cid, 'limit': 1000, 'record_open_time': 0, 'count_folders': 0})
            if res.get('data'):
                for item in res['data']:
                    # å…¼å®¹ OpenAPI é”®å
                    fc_val = item.get('fc') if item.get('fc') is not None else item.get('type')
                    if str(fc_val) == '1':
                        all_files.append(item)
                    elif str(fc_val) == '0':
                        sub_id = item.get('fid') or item.get('file_id')
                        sub_files = self._scan_files_recursively(sub_id, depth + 1, max_depth)
                        all_files.extend(sub_files)
        except Exception as e:
            logger.warning(f"  âš ï¸ æ‰«æç›®å½•å‡ºé”™ (CID: {cid}): {e}")
        return all_files

    def _is_junk_file(self, filename):
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºåƒåœ¾æ–‡ä»¶/æ ·æœ¬/èŠ±çµ® (åŸºäº MP è§„åˆ™)
        """
        # åƒåœ¾æ–‡ä»¶æ­£åˆ™åˆ—è¡¨ (åˆå¹¶äº†é€šç”¨è§„åˆ™å’Œä½ æä¾›çš„ MP è§„åˆ™)
        junk_patterns = [
            # åŸºç¡€å…³é”®è¯
            r'(?i)\b(sample|trailer|featurette|bonus)\b',

            # MP è§„åˆ™é›†
            r'(?i)Special Ending Movie',
            r'(?i)\[((TV|BD|\bBlu-ray\b)?\s*CM\s*\d{2,3})\]',
            r'(?i)\[Teaser.*?\]',
            r'(?i)\[PV.*?\]',
            r'(?i)\[NC[OPED]+.*?\]',
            r'(?i)\[S\d+\s+Recap(\s+\d+)?\]',
            r'(?i)Menu',
            r'(?i)Preview',
            r'(?i)\b(CDs|SPs|Scans|Bonus|æ˜ åƒç‰¹å…¸|æ˜ åƒ|specials|ç‰¹å…¸CD|Menu|Logo|Preview|/mv)\b',
            r'(?i)\b(NC)?(Disc|ç‰‡å¤´|OP|SP|ED|Advice|Trailer|BDMenu|ç‰‡å°¾|PV|CM|Preview|MENU|Info|EDPV|SongSpot|BDSpot)(\d{0,2}|_ALL)\b',
            r'(?i)WiKi\.sample'
        ]

        for pattern in junk_patterns:
            if re.search(pattern, filename):
                return True
        return False

    def execute(self, root_item, target_cid, delete_source=True):
        title = self.details.get('title') or self.original_title
        date_str = self.details.get('date') or ''
        year = date_str[:4] if date_str else ''

        safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip()
        std_root_name = f"{safe_title} ({year}) {{tmdb={self.tmdb_id}}}" if year else f"{safe_title} {{tmdb={self.tmdb_id}}}"

        # å…¼å®¹ OpenAPI é”®å
        root_name = root_item.get('fn') or root_item.get('n') or root_item.get('file_name', 'æœªçŸ¥')
        source_root_id = root_item.get('fid') or root_item.get('file_id')
        fc_val = root_item.get('fc') if root_item.get('fc') is not None else root_item.get('type')
        is_source_file = str(fc_val) == '1'
        dest_parent_cid = target_cid if (target_cid and str(target_cid) != '0') else (root_item.get('pid') or root_item.get('parent_id') or root_item.get('cid'))

        config = get_config()
        configured_exts = config.get(constants.CONFIG_OPTION_115_EXTENSIONS, [])
        allowed_exts = set(e.lower() for e in configured_exts)
        known_video_exts = {'mp4', 'mkv', 'avi', 'ts', 'iso', 'rmvb', 'wmv', 'mov', 'm2ts', 'flv', 'mpg'}
        MIN_VIDEO_SIZE = 10 * 1024 * 1024

        logger.info(f"  ğŸš€ [115] å¼€å§‹æ•´ç†: {root_name} -> {std_root_name}")

        final_home_cid = P115CacheManager.get_cid(dest_parent_cid, std_root_name)

        if final_home_cid:
            logger.info(f"  âš¡ [ç¼“å­˜å‘½ä¸­] ä¸»ç›®å½•: {std_root_name}")
        else:
            mk_res = self.client.fs_mkdir(std_root_name, dest_parent_cid)
            if mk_res.get('state'):
                final_home_cid = mk_res.get('cid')
                P115CacheManager.save_cid(final_home_cid, dest_parent_cid, std_root_name)
                logger.info(f"  ğŸ†• åˆ›å»ºæ–°ä¸»ç›®å½•å¹¶ç¼“å­˜: {std_root_name}")
            else:
                try:
                    search_res = self.client.fs_files({'cid': dest_parent_cid, 'search_value': std_root_name, 'limit': 1150, 'record_open_time': 0, 'count_folders': 0})
                    if search_res.get('data'):
                        for item in search_res['data']:
                            item_name = item.get('fn') or item.get('n') or item.get('file_name')
                            item_fc = item.get('fc') if item.get('fc') is not None else item.get('type')
                            if item_name == std_root_name and str(item_fc) == '0':
                                final_home_cid = item.get('fid') or item.get('file_id')
                                P115CacheManager.save_cid(final_home_cid, dest_parent_cid, std_root_name)
                                logger.info(f"  ğŸ“‚ æˆåŠŸæŸ¥æ‰¾åˆ°å·²å­˜åœ¨ä¸»ç›®å½•å¹¶æ°¸ä¹…ç¼“å­˜: {std_root_name}")
                                break
                except Exception as e:
                    logger.warning(f"  âš ï¸ 115æ¨¡ç³ŠæŸ¥æ‰¾å¼‚å¸¸: {e}")

                if not final_home_cid:
                    logger.warning(f"  âš ï¸ 115æœç´¢å¤±æ•ˆï¼Œå¯åŠ¨å…¨é‡éå†æŸ¥æ‰¾è€ç›®å½•: '{std_root_name}' ...")
                    offset = 0
                    limit = 1000
                    while True:
                        try:
                            res = self.client.fs_files({'cid': dest_parent_cid, 'limit': limit, 'offset': offset, 'type': 0, 'record_open_time': 0, 'count_folders': 0})
                            data = res.get('data', [])
                            if not data: break 
                            
                            for item in data:
                                item_name = item.get('fn') or item.get('n') or item.get('file_name')
                                item_fc = item.get('fc') if item.get('fc') is not None else item.get('type')
                                if item_name == std_root_name and str(item_fc) == '0':
                                    final_home_cid = item.get('fid') or item.get('file_id')
                                    P115CacheManager.save_cid(final_home_cid, dest_parent_cid, std_root_name)
                                    logger.info(f"  ğŸ“‚ æˆåŠŸæŸ¥æ‰¾åˆ°å·²å­˜åœ¨ä¸»ç›®å½•å¹¶æ°¸ä¹…ç¼“å­˜: {std_root_name}")
                                    break
                                    
                            if final_home_cid: break 
                            offset += limit 
                        except Exception as e:
                            logger.error(f"éå†æŸ¥æ‰¾å¤±è´¥: {e}")
                            break

        if not final_home_cid:
            logger.error(f"  âŒ æ— æ³•è·å–æˆ–åˆ›å»ºç›®æ ‡ç›®å½• (å·²å°è¯•æ‰€æœ‰æ‰‹æ®µ)")
            return False

        candidates = []
        if is_source_file:
            candidates.append(root_item)
        else:
            candidates = self._scan_files_recursively(source_root_id, max_depth=3)

        if not candidates: return True

        moved_count = 0
        for file_item in candidates:
            # å…¼å®¹ OpenAPI é”®å
            fid = file_item.get('fid') or file_item.get('file_id')
            file_name = file_item.get('fn') or file_item.get('n') or file_item.get('file_name', '')
            ext = file_name.split('.')[-1].lower() if '.' in file_name else ''
            if self._is_junk_file(file_name): continue
            if ext not in allowed_exts: continue
            
            file_size = _parse_115_size(file_item.get('fs') or file_item.get('size'))
            if ext in known_video_exts and 0 < file_size < MIN_VIDEO_SIZE: continue

            new_filename, season_num = self._rename_file_node(
                file_item, safe_title, year=year, is_tv=(self.media_type=='tv')
            )

            real_target_cid = final_home_cid
            if self.media_type == 'tv' and season_num is not None:
                s_name = f"Season {season_num:02d}"
                s_cid = P115CacheManager.get_cid(final_home_cid, s_name)
                
                if s_cid:
                    logger.info(f"  âš¡ [ç¼“å­˜å‘½ä¸­] å­£ç›®å½•: {std_root_name} - {s_name}")
                    real_target_cid = s_cid
                else:
                    s_mk = self.client.fs_mkdir(s_name, final_home_cid)
                    s_cid = s_mk.get('cid') if s_mk.get('state') else None
                    
                    if not s_cid: 
                        try:
                            s_search = self.client.fs_files({'cid': final_home_cid, 'search_value': s_name, 'limit': 1150, 'record_open_time': 0, 'count_folders': 0})
                            for item in s_search.get('data', []):
                                item_name = item.get('fn') or item.get('n') or item.get('file_name')
                                item_fc = item.get('fc') if item.get('fc') is not None else item.get('type')
                                if item_name == s_name and str(item_fc) == '0':
                                    s_cid = item.get('fid') or item.get('file_id')
                                    break
                        except: pass
                    
                    if s_cid:
                        P115CacheManager.save_cid(s_cid, final_home_cid, s_name)
                        logger.info(f"  ğŸ†• åˆ›å»ºå­£ç›®å½•å¹¶ç¼“å­˜: {std_root_name} - {s_name}")
                        real_target_cid = s_cid

            if new_filename != file_name:
                if self.client.fs_rename((fid, new_filename)).get('state'):
                    logger.info(f"  âœï¸ [é‡å‘½å] {file_name} -> {new_filename}")

            if self.client.fs_move(fid, real_target_cid).get('state'):
                if self.media_type == 'tv' and season_num is not None:
                    logger.info(f"  ğŸ“ [ç§»åŠ¨] {file_name} -> {std_root_name} - {s_name}")
                else:
                    logger.info(f"  ğŸ“ [ç§»åŠ¨] {file_name} -> {std_root_name}")
                moved_count += 1

                # å…¼å®¹ OpenAPI é”®å
                pick_code = file_item.get('pc') or file_item.get('pick_code')
                local_root = config.get(constants.CONFIG_OPTION_LOCAL_STRM_ROOT)
                etk_url = config.get(constants.CONFIG_OPTION_ETK_SERVER_URL, "http://127.0.0.1:5257").rstrip('/')
                
                if pick_code and local_root and os.path.exists(local_root):
                    try:
                        category_name = None
                        for rule in self.rules:
                            if rule.get('cid') == str(target_cid):
                                category_name = rule.get('dir_name', 'æœªè¯†åˆ«')
                                break
                        if not category_name: category_name = "æœªè¯†åˆ«"

                        category_rule = next((r for r in self.rules if str(r.get('cid')) == str(target_cid)), None)
                        
                        if category_rule and 'category_path' in category_rule:
                            relative_category_path = category_rule['category_path']
                            logger.debug(f"  âš¡ [è§„åˆ™ç¼“å­˜] åˆ†ç±»è·¯å¾„: '{relative_category_path}'")
                        else:
                            relative_category_path = category_rule.get('dir_name', 'æœªè¯†åˆ«') if category_rule else "æœªè¯†åˆ«"

                        if self.media_type == 'tv' and season_num is not None:
                            local_dir = os.path.join(local_root, relative_category_path, std_root_name, s_name)
                        else:
                            local_dir = os.path.join(local_root, relative_category_path, std_root_name)
                        
                        os.makedirs(local_dir, exist_ok=True) 

                        ext = new_filename.split('.')[-1].lower() if '.' in new_filename else ''
                        is_video = ext in known_video_exts
                        is_sub = ext in ['srt', 'ass', 'ssa', 'sub', 'vtt', 'sup']

                        if is_video:
                            strm_filename = os.path.splitext(new_filename)[0] + ".strm"
                            strm_filepath = os.path.join(local_dir, strm_filename)
                            strm_content = f"{etk_url}/api/p115/play/{pick_code}"
                            
                            with open(strm_filepath, 'w', encoding='utf-8') as f:
                                f.write(strm_content)
                            logger.info(f"  ğŸ“ STRM å·²ç”Ÿæˆ -> {strm_filename}")
                            
                        elif is_sub:
                            if config.get(constants.CONFIG_OPTION_115_DOWNLOAD_SUBS, True):
                                sub_filepath = os.path.join(local_dir, new_filename)
                                if not os.path.exists(sub_filepath):
                                    try:
                                        logger.info(f"  â¬‡ï¸ [å­—å¹•ä¸‹è½½] æ­£åœ¨å‘ 115 æ‹‰å–å¤–æŒ‚å­—å¹•: {new_filename} ...")
                                        url_obj = self.client.download_url(pick_code, user_agent="Mozilla/5.0")
                                        dl_url = str(url_obj)
                                        if dl_url:
                                            import requests
                                            headers = {
                                                "User-Agent": "Mozilla/5.0",
                                                "Cookie": self.get_cookies()
                                            }
                                            resp = requests.get(dl_url, stream=True, timeout=30, headers=headers)
                                            resp.raise_for_status()
                                            with open(sub_filepath, 'wb') as f:
                                                for chunk in resp.iter_content(chunk_size=8192):
                                                    f.write(chunk)
                                            logger.info(f"  âœ… [å­—å¹•ä¸‹è½½] ä¸‹è½½å®Œæˆï¼")
                                    except Exception as e:
                                        logger.error(f"  âŒ ä¸‹è½½å­—å¹•å¤±è´¥: {e}")
                        
                    except Exception as e:
                        logger.error(f"  âŒ ç”Ÿæˆ STRM æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)

        if delete_source and not is_source_file and moved_count > 0:
            self.client.fs_delete([source_root_id])
            logger.info(f"  ğŸ§¹ å·²æ¸…ç†ç©ºç›®å½•")

        return True

def _parse_115_size(size_val):
    """
    ç»Ÿä¸€è§£æ 115 è¿”å›çš„æ–‡ä»¶å¤§å°ä¸ºå­—èŠ‚(Int)
    æ”¯æŒ: 12345(int), "12345"(str), "1.2GB", "500KB"
    """
    try:
        if size_val is None: return 0

        # 1. å¦‚æœå·²ç»æ˜¯æ•°å€¼ (115 API 's' å­—æ®µé€šå¸¸æ˜¯ int)
        if isinstance(size_val, (int, float)):
            return int(size_val)

        # 2. å¦‚æœæ˜¯å­—ç¬¦ä¸²
        if isinstance(size_val, str):
            s = size_val.strip()
            if not s: return 0
            # çº¯æ•°å­—å­—ç¬¦ä¸²
            if s.isdigit():
                return int(s)

            s_upper = s.upper().replace(',', '')
            mult = 1
            if 'TB' in s_upper: mult = 1024**4
            elif 'GB' in s_upper: mult = 1024**3
            elif 'MB' in s_upper: mult = 1024**2
            elif 'KB' in s_upper: mult = 1024

            match = re.search(r'([\d\.]+)', s_upper)
            if match:
                return int(float(match.group(1)) * mult)
    except Exception:
        pass
    return 0

def get_115_account_info():
    """
    è·å– 115 è´¦å·çŠ¶æ€åŠè¯¦ç»†ä¿¡æ¯
    """
    client = P115Service.get_client()
    if not client: raise Exception("æ— æ³•åˆå§‹åŒ– 115 å®¢æˆ·ç«¯")

    config = get_config()
    auth_str = config.get(constants.CONFIG_OPTION_115_COOKIES, "")

    if not auth_str:
        raise Exception("æœªé…ç½® 115 å‡­è¯")

    try:
        # å°è¯•è·å–è¯¦ç»†ç”¨æˆ·ä¿¡æ¯ (ä»… OpenAPI æ”¯æŒ)
        if hasattr(client, 'get_user_info'):
            user_resp = client.get_user_info()
            if user_resp and user_resp.get('state'):
                return {
                    "valid": True,
                    "msg": "æ··åˆæ¨¡å¼æ­£å¸¸ (OpenAPI+Cookie)" if "|||" in auth_str else "OpenAPI æ¨¡å¼æ­£å¸¸",
                    "user_info": user_resp.get('data', {})
                }

        # å¦‚æœæ²¡æœ‰ OpenAPIï¼Œå›é€€åˆ°åŸºç¡€æ£€æŸ¥
        resp = client.fs_files_app({'limit': 1})
        if not resp.get('state'):
            raise Exception("å‡­è¯å·²å¤±æ•ˆ")

        return {
            "valid": True,
            "msg": "Cookie æ¨¡å¼æ­£å¸¸",
            "user_info": None
        }
    except Exception as e:
        raise Exception(f"å‡­è¯æ— æ•ˆæˆ–ç½‘ç»œä¸é€š: {e}")


def _identify_media_enhanced(filename, forced_media_type=None):
    """
    å¢å¼ºè¯†åˆ«é€»è¾‘ï¼š
    1. æ”¯æŒå¤šç§ TMDb ID æ ‡ç­¾æ ¼å¼: {tmdb=xxx}
    2. æ”¯æŒæ ‡å‡†å‘½åæ ¼å¼: Title (Year)
    3. æ¥æ”¶å¤–éƒ¨å¼ºåˆ¶æŒ‡å®šçš„ç±»å‹ (forced_media_type)ï¼Œä¸å†è½®è¯¢çŒœæµ‹
    
    è¿”å›: (tmdb_id, media_type, title) æˆ– (None, None, None)
    """
    tmdb_id = None
    media_type = 'movie' # é»˜è®¤
    title = filename
    
    # 1. ä¼˜å…ˆæå– TMDb ID æ ‡ç­¾ (æœ€ç¨³)
    match_tag = re.search(r'\{?tmdb(?:id)?[=\-](\d+)\}?', filename, re.IGNORECASE)
    
    if match_tag:
        tmdb_id = match_tag.group(1)
        
        # å¦‚æœå¤–éƒ¨æŒ‡å®šäº†ç±»å‹ï¼Œç›´æ¥ç”¨ï¼›å¦åˆ™çœ‹æ–‡ä»¶åç‰¹å¾
        if forced_media_type:
            media_type = forced_media_type
        elif re.search(r'(?:S\d{1,2}|E\d{1,2}|ç¬¬\d+å­£|Season)', filename, re.IGNORECASE):
            media_type = 'tv'
        
        # æå–æ ‡é¢˜
        clean_name = re.sub(r'\{?tmdb(?:id)?[=\-]\d+\}?', '', filename, flags=re.IGNORECASE).strip()
        match_title = re.match(r'^(.+?)\s*[\(\[]\d{4}[\)\]]', clean_name)
        if match_title:
            title = match_title.group(1).strip()
        else:
            title = clean_name
            
        return tmdb_id, media_type, title

    # 2. å…¶æ¬¡æå–æ ‡å‡†æ ¼å¼ Title (Year)
    match_std = re.match(r'^(.+?)\s+[\(\[](\d{4})[\)\]]', filename)
    if match_std:
        name_part = match_std.group(1).strip()
        year_part = match_std.group(2)
        
        # === å…³é”®ä¿®æ­£ï¼šç±»å‹åˆ¤æ–­é€»è¾‘ ===
        if forced_media_type:
            # å¦‚æœå¤–éƒ¨é€è§†è¿‡ç›®å½•ï¼Œç¡®å®šæ˜¯ TVï¼Œç›´æ¥ä¿¡èµ–
            media_type = forced_media_type
        else:
            # å¦åˆ™æ‰æ ¹æ®æ–‡ä»¶åç‰¹å¾åˆ¤æ–­
            if re.search(r'(?:S\d{1,2}|E\d{1,2}|ç¬¬\d+å­£|Season)', filename, re.IGNORECASE):
                media_type = 'tv'
            else:
                media_type = 'movie'
            
        # å°è¯•é€šè¿‡ TMDb API ç¡®è®¤ ID
        try:
            api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)
            if api_key:
                # ç²¾å‡†æœç´¢ï¼Œä¸è½®è¯¢ï¼Œä¸ççŒœ
                results = tmdb.search_media(
                    query=name_part, 
                    api_key=api_key, 
                    item_type=media_type, 
                    year=year_part
                )
                
                if results and len(results) > 0:
                    best = results[0]
                    return best['id'], media_type, (best.get('title') or best.get('name'))
                else:
                    logger.warning(f"  âš ï¸ TMDb æœªæ‰¾åˆ°èµ„æº: {name_part} ({year_part}) ç±»å‹: {media_type}")

        except Exception as e:
            pass

    return None, None, None


def task_scan_and_organize_115(processor=None):
    """
    [ä»»åŠ¡é“¾] ä¸»åŠ¨æ‰«æ 115 å¾…æ•´ç†ç›®å½•
    - è¯†åˆ«æˆåŠŸ -> å½’ç±»åˆ°ç›®æ ‡ç›®å½•
    - è¯†åˆ«å¤±è´¥ -> ç§»åŠ¨åˆ° 'æœªè¯†åˆ«' ç›®å½•
    â˜… ä¿®å¤ï¼šå¢åŠ å­æ–‡ä»¶æ¢æµ‹é€»è¾‘ï¼Œé˜²æ­¢å‰§é›†æ–‡ä»¶å¤¹å› å‘½åä¸è§„èŒƒè¢«è¯¯åˆ¤ä¸ºç”µå½±
    """
    logger.info("=== å¼€å§‹æ‰§è¡Œ 115 å¾…æ•´ç†ç›®å½•æ‰«æ ===")

    client = P115Service.get_client()
    if not client: raise Exception("æ— æ³•åˆå§‹åŒ– 115 å®¢æˆ·ç«¯")

    config = get_config()
    cookies = config.get(constants.CONFIG_OPTION_115_COOKIES)
    cid_val = config.get(constants.CONFIG_OPTION_115_SAVE_PATH_CID)
    save_val = config.get(constants.CONFIG_OPTION_115_SAVE_PATH_NAME, 'å¾…æ•´ç†')
    enable_organize = config.get(constants.CONFIG_OPTION_115_ENABLE_ORGANIZE, False)

    if not cookies:
        logger.error("  âš ï¸ æœªé…ç½® 115 Cookiesï¼Œè·³è¿‡ã€‚")
        return
    if not cid_val or str(cid_val) == '0':
        logger.error("  âš ï¸ æœªé…ç½®å¾…æ•´ç†ç›®å½• (CID)ï¼Œè·³è¿‡ã€‚")
        return
    if not enable_organize:
        logger.warning("  âš ï¸ æœªå¼€å¯æ™ºèƒ½æ•´ç†å¼€å…³ï¼Œä»…æ‰«æä¸å¤„ç†ã€‚")
        return
    current_time = time.time()
    try:
        save_cid = int(cid_val)
        save_name = str(save_val)

        # 1. å‡†å¤‡ 'æœªè¯†åˆ«' ç›®å½•
        unidentified_folder_name = "æœªè¯†åˆ«"
        unidentified_cid = None
        try:
            time.sleep(1.5)
            # â˜… ä¼˜åŒ–ï¼šçº¯è¯»æ¨¡å¼ï¼Œä¸ç»Ÿè®¡æ–‡ä»¶å¤¹
            search_res = client.fs_files({
                'cid': save_cid, 'search_value': unidentified_folder_name, 'limit': 1,
                'record_open_time': 0, 'count_folders': 0
            })
            if search_res.get('data'):
                for item in search_res['data']:
                    if item.get('fn') == unidentified_folder_name and str(item.get('fc')) == '0':
                        unidentified_cid = item.get('fid')
                        break
        except: pass

        if not unidentified_cid:
            try:
                mk_res = client.fs_mkdir(unidentified_folder_name, save_cid)
                if mk_res.get('state'): unidentified_cid = mk_res.get('cid')
            except: pass

        logger.info(f"  ğŸ” æ­£åœ¨æ‰«æç›®å½•: {save_name} ...")
        
        # =================================================================
        # â˜…â˜…â˜… ä¸»ç›®å½•æ‰«æï¼šçº¯è¯»æ¨¡å¼ + ä¿®æ­£æ’åºå­—æ®µ + é€€é¿é‡è¯• â˜…â˜…â˜…
        # =================================================================
        res = {}
        for retry in range(3):
            try:
                time.sleep(2)
                res = client.fs_files({
                    'cid': save_cid, 'limit': 50, 'o': 'user_utime', 'asc': 0,
                    'record_open_time': 0, 'count_folders': 0
                })
                break 
            except Exception as e:
                if '405' in str(e) or 'Method Not Allowed' in str(e):
                    logger.warning(f"  âš ï¸ æ‰«æä¸»ç›®å½•è§¦å‘ 115 é£æ§æ‹¦æˆª (405)ï¼Œä¼‘çœ  5 ç§’åé‡è¯• ({retry+1}/3)...")
                    time.sleep(5)
                else:
                    raise

        if not res.get('data'):
            logger.info(f"  ğŸ“‚ [{save_name}] ç›®å½•ä¸ºç©ºæˆ–è·å–å¤±è´¥ã€‚")
            return

        processed_count = 0
        moved_to_unidentified = 0

        for item in res['data']:
            # å…¼å®¹ OpenAPI é”®å
            name = item.get('fn') or item.get('n') or item.get('file_name')
            if not name: continue
            item_id = item.get('fid') or item.get('file_id')
            fc_val = item.get('fc') if item.get('fc') is not None else item.get('type')
            is_folder = str(fc_val) == '0'

            if str(item_id) == str(unidentified_cid) or name == unidentified_folder_name:
                continue

            forced_type = None
            peek_failed = False

            if is_folder:
                # =================================================================
                # â˜…â˜…â˜… å­ç›®å½•é€è§†ï¼šå¼€å¯ nf=1 (ä»…çœ‹æ–‡ä»¶å¤¹) æå¤§é™ä½è´Ÿè½½ â˜…â˜…â˜…
                # =================================================================
                for retry in range(2):
                    try:
                        time.sleep(2)
                        sub_res = client.fs_files({
                            'cid': item.get('cid'), 'limit': 20, 
                            'nf': 1, # â˜… æ ¸å¿ƒä¼˜åŒ–ï¼šåªè¿”å›æ–‡ä»¶å¤¹ï¼Œä¸è¿”å›æ–‡ä»¶
                            'record_open_time': 0, 'count_folders': 0
                        })
                        if sub_res.get('data'):
                            for sub_item in sub_res['data']:
                                sub_name = sub_item.get('fn', '')
                                if re.search(r'(Season\s?\d+|S\d+|Ep?\d+|ç¬¬\d+å­£)', sub_name, re.IGNORECASE):
                                    forced_type = 'tv'
                                    break
                        peek_failed = False
                        break
                    except Exception as e:
                        if '405' in str(e) or 'Method Not Allowed' in str(e):
                            logger.warning(f"  âš ï¸ é€è§†ç›®å½• '{name}' è§¦å‘é£æ§ï¼Œä¼‘çœ  3 ç§’åé‡è¯• ({retry+1}/2)...")
                            time.sleep(3)
                            peek_failed = True
                        else:
                            peek_failed = True
                            break

            if peek_failed:
                logger.warning(f"  â­ï¸ é€è§† '{name}' è¿ç»­å¤±è´¥ï¼Œä¸ºé˜²è¯¯åˆ¤è·³è¿‡æœ¬æ¬¡è¯†åˆ«ã€‚")
                continue

            tmdb_id, media_type, title = _identify_media_enhanced(name, forced_media_type=forced_type)
            
            if tmdb_id:
                logger.info(f"  âœ è¯†åˆ«æˆåŠŸ: {name} -> ID:{tmdb_id} ({media_type})")
                try:
                    organizer = SmartOrganizer(client, tmdb_id, media_type, title)
                    target_cid = organizer.get_target_cid()
                    
                    if organizer.execute(item, target_cid, delete_source=False):
                        processed_count += 1
                        
                        if is_folder:
                            update_time_str = item.get('upt') or '0'
                            try:
                                update_time = int(update_time_str)
                            except:
                                update_time = current_time
                                
                            if (current_time - update_time) > 86400:
                                logger.info(f"  ğŸ§¹ [å…œåº•æ¸…ç†] æ¸…ç†å·²è¿‡æœŸ(>24h)çš„æ®‹ç•™ç›®å½•: {name}")
                                client.fs_delete([item_id])

                except Exception as e:
                    logger.error(f"  âŒ æ•´ç†å‡ºé”™: {e}")
            else:
                if unidentified_cid:
                    try:
                        client.fs_move(item_id, unidentified_cid)
                        moved_to_unidentified += 1
                    except: pass

        logger.info(f"=== æ‰«æç»“æŸï¼ŒæˆåŠŸå½’ç±» {processed_count} ä¸ªï¼Œç§»å…¥æœªè¯†åˆ« {moved_to_unidentified} ä¸ª ===")

    except Exception as e:
        logger.error(f"  âš ï¸ 115 æ‰«æä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)

def task_sync_115_directory_tree(processor=None):
    """
    ä¸»åŠ¨åŒæ­¥ 115 åˆ†ç±»ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•åˆ°æœ¬åœ° DB ç¼“å­˜ã€‚
    è¿™èƒ½å½»åº•è§£å†³ 115 API search_value å¤±æ•ˆå¯¼è‡´çš„è€ç›®å½•æ— æ³•è¯†åˆ«é—®é¢˜ã€‚
    """
    logger.info("=== å¼€å§‹å…¨é‡åŒæ­¥ 115 ç›®å½•æ ‘åˆ°æœ¬åœ°æ•°æ®åº“ ===")
    
    # å±€éƒ¨å¯¼å…¥ task_manager ç”¨äºå‘å‰ç«¯å‘é€å®æ—¶è¿›åº¦ (é˜²æ­¢ä¸ core.py å¾ªç¯å¼•ç”¨)
    try:
        import task_manager
    except ImportError:
        task_manager = None

    def update_progress(prog, msg):
        if task_manager:
            task_manager.update_status_from_thread(prog, msg)
        logger.info(msg)

    client = P115Service.get_client()
    if not client: 
        update_progress(100, "115 å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä»»åŠ¡ç»“æŸã€‚")
        return

    raw_rules = settings_db.get_setting(constants.DB_KEY_115_SORTING_RULES)
    if not raw_rules: 
        update_progress(100, "æœªé…ç½®åˆ†ç±»è§„åˆ™ï¼Œæ— éœ€åŒæ­¥ã€‚")
        return
    
    rules = json.loads(raw_rules) if isinstance(raw_rules, str) else raw_rules
    
    # æå–æ‰€æœ‰å¯ç”¨çš„è§„åˆ™ä¸­çš„ç›®æ ‡åˆ†ç±»ç›®å½• CIDï¼Œå¹¶å»é‡
    target_cids = set()
    for rule in rules:
        if rule.get('enabled', True) and rule.get('cid'):
            cid_str = str(rule['cid'])
            if cid_str and cid_str != '0':
                target_cids.add(cid_str)

    if not target_cids:
        update_progress(100, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ç±»ç›®æ ‡ç›®å½• CIDï¼Œä»»åŠ¡ç»“æŸã€‚")
        return

    total_cached = 0
    total_cids = len(target_cids)
    
    for idx, cid in enumerate(target_cids):
        base_prog = int((idx / total_cids) * 100)
        update_progress(base_prog, f"  ğŸ” æ­£åœ¨æ‰«æç¬¬ {idx+1}/{total_cids} ä¸ªåˆ†ç±»ç›®å½• (CID: {cid})...")
        
        offset = 0
        limit = 1000
        page_count = 0
        
        while True:
            # å“åº”å‰ç«¯çš„ä¸­æ­¢ä»»åŠ¡æŒ‰é’®
            if processor and getattr(processor, 'is_stop_requested', lambda: False)():
                update_progress(100, "ä»»åŠ¡å·²è¢«ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ã€‚")
                return

            try:
                # è·å–æ•°æ®åˆ—è¡¨
                res = client.fs_files({'cid': cid, 'limit': limit, 'offset': offset, 'record_open_time': 0, 'count_folders': 0})
                data = res.get('data', [])
                
                if not data: 
                    break # æœ¬ç›®å½•å…¨ç©ºï¼Œè·³å‡º
                
                page_count += 1
                dir_count_in_page = 0
                
                with get_db_connection() as conn:
                    with conn.cursor() as cursor:
                        for item in data:
                            # å…¼å®¹ OpenAPI é”®å
                            fc_val = item.get('fc') if item.get('fc') is not None else item.get('type')
                            if str(fc_val) == '0':
                                sub_cid = item.get('fid') or item.get('file_id')
                                sub_name = item.get('fn') or item.get('n') or item.get('file_name')
                                if sub_cid and sub_name:
                                    cursor.execute("""
                                        INSERT INTO p115_filesystem_cache (id, parent_id, name)
                                        VALUES (%s, %s, %s)
                                        ON CONFLICT (parent_id, name)
                                        DO UPDATE SET id = EXCLUDED.id, updated_at = NOW()
                                    """, (str(sub_cid), str(cid), str(sub_name)))
                                    total_cached += 1
                                    dir_count_in_page += 1
                        conn.commit()
                
                # å®æ—¶æ’­æŠ¥å½“å‰æ­£åœ¨ç¿»ç¬¬å‡ é¡µï¼Œä»¥åŠå…¥åº“äº†å¤šå°‘ä¸ªæ–‡ä»¶å¤¹
                update_progress(base_prog, f"  âœ CID: {cid} | ç¿»é˜…ç¬¬ {page_count} é¡µ | æ–°å¢/æ›´æ–° {dir_count_in_page} ä¸ªç›®å½•...")
                
                # â˜… æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æœè·å–çš„æ•°æ®å°äºè¯·æ±‚çš„ä¸Šé™ï¼Œè¯´æ˜åˆ°åº•äº†ï¼Œä¸ç”¨å†è¯·æ±‚ä¸‹ä¸€é¡µ
                if len(data) < limit:
                    break
                    
                offset += limit
                time.sleep(1) # ç¨å¾®å–˜å£æ°”ï¼Œé˜² 115 è¸¢äºº
                
            except Exception as e:
                logger.error(f"  âŒ åŒæ­¥ç›®å½•æ ‘å¼‚å¸¸ (CID: {cid}): {e}")
                break # å‘ç”Ÿå¼‚å¸¸ï¼Œè·³è¿‡è¿™ä¸ª CID ç»§ç»­æŸ¥ä¸‹ä¸€ä¸ª

    update_progress(100, f"=== åŒæ­¥ç»“æŸï¼å…±æˆåŠŸæ›´æ–° {total_cached} ä¸ªç›®å½•çš„ç¼“å­˜ ===")

def task_full_sync_strm_and_subs(processor=None):
    """
    æé€Ÿå…¨é‡ç”Ÿæˆ STRM ä¸ åŒæ­¥å­—å¹• (å¸¦é˜²å¤±è´¥è‡ªåŠ¨é™çº§æœºåˆ¶)
    ä¿®å¤ç‰ˆï¼šå®Œç¾å¯¹é½ç½‘ç›˜ä¸æœ¬åœ°åˆ†ç±»ç›®å½•çš„å±‚çº§è·¯å¾„
    """
    config = get_config()
    download_subs = config.get(constants.CONFIG_OPTION_115_DOWNLOAD_SUBS, True)
    enable_cleanup = config.get(constants.CONFIG_OPTION_115_LOCAL_CLEANUP, False)
    start_msg = "=== ğŸš€ å¼€å§‹å…¨é‡ç”Ÿæˆ STRM ä¸ åŒæ­¥å­—å¹• ===" if download_subs else "=== ğŸš€ å¼€å§‹å…¨é‡ç”Ÿæˆ STRM (å·²è·³è¿‡å­—å¹•) ==="
    if enable_cleanup: start_msg += " [å·²å¼€å¯æœ¬åœ°æ¸…ç†]"
    logger.info(start_msg)
    
    try:
        import task_manager
    except ImportError:
        task_manager = None

    def update_progress(prog, msg):
        if task_manager: task_manager.update_status_from_thread(prog, msg)
        logger.info(msg)

    local_root = config.get(constants.CONFIG_OPTION_LOCAL_STRM_ROOT)
    etk_url = config.get(constants.CONFIG_OPTION_ETK_SERVER_URL, "").rstrip('/')
    media_root_cid = str(config.get(constants.CONFIG_OPTION_115_MEDIA_ROOT_CID, '0'))
    
    known_video_exts = {'mp4', 'mkv', 'avi', 'ts', 'iso', 'rmvb', 'wmv', 'mov', 'm2ts', 'flv', 'mpg'}
    known_sub_exts = {'srt', 'ass', 'ssa', 'sub', 'vtt', 'sup'}
    
    allowed_exts = set(e.lower() for e in config.get(constants.CONFIG_OPTION_115_EXTENSIONS, []))
    if not allowed_exts:
        allowed_exts = known_video_exts | known_sub_exts
    
    if not local_root or not etk_url:
        update_progress(100, "é”™è¯¯ï¼šæœªé…ç½®æœ¬åœ° STRM æ ¹ç›®å½•æˆ– ETK è®¿é—®åœ°å€ï¼")
        return

    client = P115Service.get_client()
    if not client: return

    raw_rules = settings_db.get_setting(constants.DB_KEY_115_SORTING_RULES)
    if not raw_rules: return
    rules = json.loads(raw_rules) if isinstance(raw_rules, str) else raw_rules
    
    # 1. é¢„å¤„ç†ï¼šè·å–æ¯ä¸ªç›®æ ‡åˆ†ç±»ç›®å½•å¯¹åº”çš„å®Œæ•´ç›¸å¯¹è·¯å¾„ (å‚è€ƒ execute é€»è¾‘)
    cid_to_rel_path = {}
    target_cids = []
    
    for r in rules:
        if r.get('enabled', True) and r.get('cid') and str(r['cid']) != '0':
            cid = str(r['cid'])
            target_cids.append(cid)
            # â˜… æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥ä»è§„åˆ™ä¸­è¯»å– category_path
            if 'category_path' in r:
                cid_to_rel_path[cid] = r['category_path']
            else:
                # å…œåº•ï¼šä½¿ç”¨è§„åˆ™ä¸­é…ç½®çš„åç§°
                cid_to_rel_path[cid] = r.get('dir_name', 'æœªè¯†åˆ«')

    valid_local_files = set() # æœ¬åœ°å·²å­˜åœ¨çš„ STRM å’Œå­—å¹•æ–‡ä»¶ç»å¯¹è·¯å¾„é›†åˆï¼ˆä»…å½“ enable_cleanup=True æ—¶ä½¿ç”¨ï¼‰
    successful_cids = set() # è®°å½•æˆåŠŸå¤„ç†è¿‡çš„ CIDï¼Œæœ€åç”¨äºæ¸…ç†æœ¬åœ°å¤šä½™æ–‡ä»¶
    # ==========================================
    # â˜… å†…éƒ¨å¤„ç†é€»è¾‘ï¼šæ¥æ”¶ base_cid æ¥ç¡®å®šåˆ†ç±»å‰ç¼€
    # ==========================================
    def process_file_info(info, rel_path_parts, base_cid):
        nonlocal files_generated
        # å…¼å®¹ OpenAPI é”®å
        name = info.get('fn') or info.get('n') or info.get('file_name', '')
        ext = name.split('.')[-1].lower() if '.' in name else ''
        if ext not in allowed_exts: return
        pc = info.get('pc') or info.get('pick_code')
        if not pc: return
        
        # è·å–åˆ†ç±»å‰ç¼€è·¯å¾„ (ä¾‹å¦‚ "çºªå½•ç‰‡/BBC")
        category_prefix = cid_to_rel_path.get(str(base_cid), "æœªè¯†åˆ«")
        
        # æ‹¼æ¥æœ¬åœ°è·¯å¾„ï¼šæœ¬åœ°æ ¹ç›®å½• / åˆ†ç±»å‰ç¼€ / èµ„æºå­ç›®å½• / æ–‡ä»¶
        current_local_path = os.path.join(local_root, category_prefix, *rel_path_parts)
        os.makedirs(current_local_path, exist_ok=True)
        
        if ext in known_video_exts:
            strm_name = os.path.splitext(name)[0] + ".strm"
            strm_path = os.path.join(current_local_path, strm_name)
            content = f"{etk_url}/api/p115/play/{pc}"
            
            need_write = True
            if os.path.exists(strm_path):
                try:
                    with open(strm_path, 'r', encoding='utf-8') as f:
                        if f.read().strip() == content: need_write = False
                except: pass
                        
            if need_write:
                with open(strm_path, 'w', encoding='utf-8') as f: f.write(content)
                logger.debug(f"ç”Ÿæˆ STRM: {strm_name}")
            files_generated += 1
            valid_local_files.add(os.path.abspath(strm_path)) # è®°å½•æœ‰æ•ˆæ–‡ä»¶ç»å¯¹è·¯å¾„
                
        elif ext in known_sub_exts:
            # æ£€æŸ¥å¼€å…³
            if download_subs:
                sub_path = os.path.join(current_local_path, name)
                if not os.path.exists(sub_path):
                    try:
                        import requests
                        url_obj = client.download_url(pc, user_agent="Mozilla/5.0")
                        if url_obj:
                            headers = {
                                "User-Agent": "Mozilla/5.0",
                                "Cookie": P115Service.get_cookies()
                            }
                            resp = requests.get(str(url_obj), stream=True, timeout=15, headers=headers)
                            resp.raise_for_status()
                            with open(sub_path, 'wb') as f:
                                for chunk in resp.iter_content(8192): f.write(chunk)
                            logger.info(f"ä¸‹è½½å­—å¹•: {name}")
                        files_generated += 1
                        valid_local_files.add(os.path.abspath(sub_path)) # è®°å½•æœ‰æ•ˆæ–‡ä»¶ç»å¯¹è·¯å¾„
                    except Exception as e:
                        logger.error(f"ä¸‹è½½å­—å¹•å¤±è´¥ [{name}]: {e}")

    # ==========================================
    # 2. éå†æ‰§è¡Œ
    # ==========================================
    total_cids = len(target_cids)
    for idx, base_cid in enumerate(target_cids):
        base_prog = int((idx / total_cids) * 100)
        category_rel_path = cid_to_rel_path.get(base_cid)
        update_progress(base_prog, f"  âœ æ­£åœ¨åŒæ­¥å±‚çº§: {category_rel_path} (CID: {base_cid}) ...")
        
        items_yielded = 0
        files_generated = 0
        
        # A. ä¼˜å…ˆå°è¯•æé€Ÿéå†
        try:
            from p115client.tool.iterdir import iter_files_with_path_skim
            
            iterator = iter_files_with_path_skim(
                client, 
                int(base_cid), 
                with_ancestors=True, 
                max_workers=1 
            )
            
            for info in iterator:
                if processor and getattr(processor, 'is_stop_requested', lambda: False)():
                    update_progress(100, "ä»»åŠ¡å·²è¢«ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ã€‚")
                    return
                
                # åªæœ‰å¸¦ fid çš„æ‰æ˜¯æ–‡ä»¶ï¼Œæ–‡ä»¶å¤¹ä¸å‚ä¸ process_file_info
                fid = info.get('fid') or info.get('id')
                if not fid or info.get('ico') == 'folder':
                    continue

                items_yielded += 1
                
                ancestors = info.get('ancestors', [])
                rel_path_parts = []
                
                if isinstance(ancestors, list) and len(ancestors) > 0:
                    found_base = False
                    for node in ancestors:
                        node_id = str(node.get('id') or node.get('cid', ''))
                        
                        # æ‰¾åˆ°è§„åˆ™é…ç½®çš„æ ¹ CID
                        if node_id == str(base_cid):
                            found_base = True
                            continue
                        
                        if found_base:
                            # ä¿®å¤ç‚¹ 1ï¼šç¡®ä¿è¿™ä¸ªèŠ‚ç‚¹ä¸æ˜¯æ–‡ä»¶æœ¬èº«ï¼ˆé˜²æ­¢æé€Ÿæ¨¡å¼æŠŠæ–‡ä»¶å½“è·¯å¾„ï¼‰
                            node_name = str(node.get('name', '')).strip()
                            if node_id != str(fid) and node_name:
                                rel_path_parts.append(node_name)
                
                # ä¿®å¤ç‚¹ 2ï¼šåŒé‡ä¿é™©ã€‚å¦‚æœè·¯å¾„æœ€åä¸€ä½è·Ÿæ–‡ä»¶åå®Œå…¨ä¸€æ ·ï¼ˆæ¯”å¦‚ 115 é‡Œçš„ç‰¹æ®Šæ‰“åŒ…æ–‡ä»¶ï¼‰ï¼Œå‰”é™¤å®ƒ
                file_real_name = info.get('n') or info.get('name', '')
                if rel_path_parts and rel_path_parts[-1] == file_real_name:
                    rel_path_parts.pop()

                process_file_info(info, rel_path_parts, base_cid)
                
        except Exception as e:
            logger.warning(f"  âš ï¸ æé€Ÿéå†å¼‚å¸¸ CID:{base_cid} - é”™è¯¯è¯¦æƒ…: {repr(e)}")

        # B. è‡ªåŠ¨é™çº§ï¼šå¦‚æœæé€Ÿæ¨¡å¼æ²¡å‡ºè´§ï¼Œå¯åŠ¨æ ‡å‡†é€’å½’
        if items_yielded == 0:
            logger.warning(f"  âš ï¸ æé€Ÿéå†æœªå‘ç°æ–‡ä»¶ï¼Œæ­£åœ¨ä½¿ç”¨æ ‡å‡†é€’å½’æ‰«æ...")
            def reliable_recursive_scan(cid, current_parts):
                offset = 0
                limit = 1000
                while True:
                    if processor and getattr(processor, 'is_stop_requested', lambda: False)(): return
                    res = client.fs_files({'cid': cid, 'limit': limit, 'offset': offset, 'record_open_time': 0, 'count_folders': 0})
                    data = res.get('data', [])
                    if not data: break
                    for item in data:
                        if str(item.get('fc')) == '1':
                            process_file_info(item, current_parts, base_cid)
                        elif str(item.get('fc')) == '0':
                            reliable_recursive_scan(item.get('fid'), current_parts + [item.get('fn')])
                    if len(data) < limit: break
                    offset += limit
            
            try:
                reliable_recursive_scan(base_cid, [])
            except Exception as e:
                logger.error(f"æ ‡å‡†æ‰«æå¼‚å¸¸ CID:{base_cid}: {e}")
                
        logger.info(f"  âœ… [{category_rel_path}] åŒæ­¥å®Œæˆï¼Œå¤„ç†æ–‡ä»¶: {files_generated}")
        if files_generated > 0:
            successful_cids.add(base_cid)
        # ==========================================
    # â˜… æ–°å¢ï¼šå®‰å…¨çš„æœ¬åœ°æ¸…ç†é€»è¾‘ (æ”¾åœ¨ for å¾ªç¯å¤–é¢ï¼Œå‡½æ•°çš„æœ«å°¾)
    # ==========================================
    if enable_cleanup:
        update_progress(95, "  ğŸ§¹ æ­£åœ¨æ‰§è¡Œæœ¬åœ°å¤šä½™æ–‡ä»¶æ¸…ç†...")
        cleaned_files = 0
        cleaned_dirs = 0
        
        for base_cid in successful_cids:
            category_rel_path = cid_to_rel_path.get(base_cid)
            target_local_dir = os.path.join(local_root, category_rel_path)
            
            if not os.path.exists(target_local_dir): continue
            
            # 1. æ¸…ç†å¤šä½™çš„æ–‡ä»¶ (åªç¢° strm å’Œ å­—å¹•)
            for root_dir, dirs, files in os.walk(target_local_dir):
                for file in files:
                    ext = file.split('.')[-1].lower()
                    if ext in known_sub_exts or ext == 'strm':
                        file_path = os.path.abspath(os.path.join(root_dir, file))
                        if file_path not in valid_local_files:
                            try:
                                os.remove(file_path)
                                cleaned_files += 1
                                logger.debug(f"  ğŸ—‘ï¸ [æ¸…ç†] åˆ é™¤å¤±æ•ˆæ–‡ä»¶: {file}")
                            except Exception as e:
                                logger.warning(f"  âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file}: {e}")
            
            # 2. æ¸…ç†ç©ºæ–‡ä»¶å¤¹ (è‡ªåº•å‘ä¸Š)
            for root_dir, dirs, files in os.walk(target_local_dir, topdown=False):
                for d in dirs:
                    dir_path = os.path.join(root_dir, d)
                    try:
                        if not os.listdir(dir_path): # å¦‚æœæ–‡ä»¶å¤¹ä¸ºç©º
                            os.rmdir(dir_path)
                            cleaned_dirs += 1
                    except: pass
                    
        logger.info(f"  ğŸ§¹ æ¸…ç†å®Œæˆ: åˆ é™¤äº† {cleaned_files} ä¸ªå¤±æ•ˆæ–‡ä»¶, {cleaned_dirs} ä¸ªç©ºç›®å½•ã€‚")

    end_msg = "=== å…¨é‡ STRM ä¸å­—å¹•åŒæ­¥ç»“æŸ ===" if download_subs else "=== å…¨é‡ STRM ç”Ÿæˆç»“æŸ ==="
    update_progress(100, end_msg)

def delete_115_files_by_webhook(item_path, pickcodes):
    """
    æ¥æ”¶ç¥åŒ» Webhook ä¼ æ¥çš„è·¯å¾„å’Œæå–ç ï¼Œç²¾å‡†é”€æ¯ 115 ç½‘ç›˜æ–‡ä»¶ã€‚
    â˜… å¢åŠ é˜²é£æ§é™æµä¸ç†”æ–­ä¿æŠ¤æœºåˆ¶
    """
    if not pickcodes or not item_path: return

    client = P115Service.get_client()
    if not client: return

    try:
        # 1. ä»æœ¬åœ°è·¯å¾„ä¸­æå–å¸¦æœ‰ TMDb ID çš„ä¸»ç›®å½•åç§° (ä¾‹å¦‚: çˆ±æˆ‘çˆ±æˆ‘ (2026) {tmdb=1317672})
        match = re.search(r'([^/\\]+\{tmdb=\d+\})', item_path)
        if not match:
            logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æ— æ³•ä»è·¯å¾„æå– TMDb ç›®å½•å: {item_path}")
            return
        tmdb_folder_name = match.group(1)

        # 2. æŸ¥æ‰¾è¯¥ä¸»ç›®å½•åœ¨ 115 ä¸Šçš„ CID
        base_cid = P115CacheManager.get_cid_by_name(tmdb_folder_name)
        if not base_cid:
            # ç¼“å­˜æ²¡å‘½ä¸­ï¼Œå°è¯•æ¨¡ç³Šæœç´¢å…œåº•
            try:
                time.sleep(1.5) # â˜… æœç´¢æ¥å£é£æ§æä¸¥ï¼Œå¿…é¡»åŠ ç¡çœ é™æµ
                res = client.fs_files({'search_value': tmdb_folder_name, 'limit': 1000, 'record_open_time': 0, 'count_folders': 0})
                for item in res.get('data', []):
                    if item.get('fn') == tmdb_folder_name and str(item.get('fc')) == '0':
                        base_cid = item.get('fid')
                        break
            except Exception as e:
                logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æ¨¡ç³Šæœç´¢ç›®å½• '{tmdb_folder_name}' æ—¶è¢«é£æ§æˆ–æŠ¥é”™: {e}")

        if not base_cid:
            logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æœªåœ¨ 115 æ‰¾åˆ°å¯¹åº”ä¸»ç›®å½•ï¼Œå¯èƒ½å·²è¢«åˆ é™¤: {tmdb_folder_name}")
            return

        # 3. é€’å½’æ‰«æè¯¥ä¸»ç›®å½•ï¼Œå°† Pickcode æ˜ å°„ä¸º 115 çš„æ–‡ä»¶ ID (fid)
        fids_to_delete = []
        
        def scan_and_match(cid):
            try:
                time.sleep(1.5) # â˜… å¼ºåˆ¶é˜²é£æ§é™æµï¼šæ¯æ¬¡è¯·æ±‚é—´éš” 1.5 ç§’
                res = client.fs_files({'cid': cid, 'limit': 1000, 'record_open_time': 0, 'count_folders': 0})
                for item in res.get('data', []):
                    if str(item.get('fc')) == '1':
                        if item.get('pc') in pickcodes:
                            fids_to_delete.append(item.get('fid'))
                    elif str(item.get('fc')) == '0':
                        scan_and_match(item.get('fid'))
            except Exception as e:
                logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æ‰«æç›®å½• {cid} æ—¶è¢«é£æ§æˆ–æŠ¥é”™: {e}")

        logger.debug(f"  ğŸ” [è”åŠ¨åˆ é™¤] æ­£åœ¨ç½‘ç›˜ç›®å½• '{tmdb_folder_name}' ä¸­åŒ¹é…æ–‡ä»¶ (å¸¦é˜²é£æ§å»¶è¿Ÿ)...")
        scan_and_match(base_cid)

        # 4. æ‰§è¡Œç‰©ç†é”€æ¯
        if fids_to_delete:
            resp = client.fs_delete(fids_to_delete)
            if resp.get('state'):
                logger.info(f"  ğŸ’¥ [è”åŠ¨åˆ é™¤] æˆåŠŸåœ¨ 115 ç½‘ç›˜åˆ é™¤äº† {len(fids_to_delete)} ä¸ªæ–‡ä»¶ï¼")
            else:
                logger.error(f"  âŒ [è”åŠ¨åˆ é™¤] 115 åˆ é™¤æ¥å£è°ƒç”¨å¤±è´¥: {resp}")

            # 5. é­å°¸æ£€æŸ¥ï¼šå¦‚æœä¸»ç›®å½•é‡Œå·²ç»æ²¡æœ‰è§†é¢‘æ–‡ä»¶äº†ï¼Œè¿ç›®å½•ä¸€èµ·æ‰¬äº†
            video_count = 0
            def count_videos(cid):
                nonlocal video_count
                try:
                    time.sleep(1.5) # â˜… å¼ºåˆ¶é˜²é£æ§é™æµ
                    res = client.fs_files({'cid': cid, 'limit': 1000, 'record_open_time': 0, 'count_folders': 0})
                    for item in res.get('data', []):
                        if str(item.get('fc')) == '1':
                            ext = str(item.get('fn', '')).split('.')[-1].lower()
                            if ext in ['mp4', 'mkv', 'avi', 'ts', 'iso']:
                                video_count += 1
                        elif str(item.get('fc')) == '0':
                            count_videos(item.get('fid'))
                except Exception as e:
                    logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æ£€æŸ¥ç©ºç›®å½• {cid} æ—¶æŠ¥é”™: {e}")
                    # â˜… ç†”æ–­ä¿æŠ¤ï¼šå¦‚æœæ¥å£æŠ¥é”™ï¼Œå‡è£…é‡Œé¢è¿˜æœ‰è§†é¢‘ï¼Œç»å¯¹ä¸æ‰§è¡Œåˆ ç›®å½•æ“ä½œï¼
                    video_count += 999 

            count_videos(base_cid)
            if video_count == 0:
                client.fs_delete(base_cid)
                P115CacheManager.delete_cid(base_cid) # æ¸…ç†æœ¬åœ°ç¼“å­˜
                logger.info(f"  ğŸ§¹ [è”åŠ¨åˆ é™¤] æ¸…ç†æœ¬åœ°ä¸»ç›®å½•ç¼“å­˜: {tmdb_folder_name}")
            else:
                logger.debug(f"  ğŸ›¡ï¸ [è”åŠ¨åˆ é™¤] ç›®å½•å†…ä»æœ‰è§†é¢‘æˆ–æ£€æŸ¥å—é˜»ï¼Œä¿ç•™ä¸»ç›®å½•ã€‚")
        else:
            logger.warning(f"  âš ï¸ [è”åŠ¨åˆ é™¤] æ‰«æå®Œæ¯•ï¼Œä½†æœªåœ¨ç½‘ç›˜æ‰¾åˆ°åŒ¹é…çš„æå–ç æ–‡ä»¶ã€‚")

    except Exception as e:
        logger.error(f"  âŒ [è”åŠ¨åˆ é™¤] æ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
